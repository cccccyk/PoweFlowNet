import os
import logging
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from torch_geometric.loader import DataLoader
from sklearn.metrics import confusion_matrix
from collections import Counter
import seaborn as sns

from datasets.PowerFlowData import PowerFlowData 
from networks.MPN import MaskEmbdMultiMPN_GPS
from utils.evaluation import load_model
from utils.argument_parser import argument_parser

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ==========================================
# 1. ç‰©ç†è¾…åŠ©å‡½æ•°
# ==========================================

def rect_to_polar(e, f):
    """å°†ç›´è§’åæ ‡é¢„æµ‹å€¼è½¬æ¢ä¸ºæåæ ‡ (Vm, Va_degree)"""
    vm = torch.sqrt(e**2 + f**2 + 1e-12)
    va_rad = torch.atan2(f, e)
    va_deg = va_rad * (180.0 / torch.pi)
    return vm, va_deg

@torch.no_grad()
def debug_max_voltage_source(model, loader, device, xymean, xystd):
    """
    ä¸“é¡¹ä½“æ£€ï¼šè¯Šæ–­å…¨ç½‘æœ€é«˜ç”µå‹(Max Vm)åˆ°åº•æ¥è‡ªå“ªé‡Œï¼Ÿ
    """
    model.eval()
    
    ef_mean = xymean[:, 2:4].to(device)
    ef_std  = xystd[:, 2:4].to(device)
    
    # å­˜å‚¨æ¯ä¸€å¼ å›¾çš„è¯Šæ–­ä¿¡æ¯
    records = []
    
    print("ğŸ•µï¸ å¼€å§‹ Max Voltage æº¯æºè¯Šæ–­...")
    
    for batch_idx, data in enumerate(loader):
        data = data.to(device)
        out = model(data)
        
        # 1. åå½’ä¸€åŒ–
        pred_real = out * (ef_std + 1e-7) + ef_mean
        target_real = data.y[:, 2:4] * (ef_std + 1e-7) + ef_mean
        
        # 2. è®¡ç®— Vm
        pred_vm = torch.sqrt(pred_real[:, 0]**2 + pred_real[:, 1]**2 + 1e-12)
        true_vm = torch.sqrt(target_real[:, 0]**2 + target_real[:, 1]**2 + 1e-12)
        
        # =========================================================
        # ğŸ›¡ï¸ æ­¥éª¤ A: å¼ºåŠ›éªŒè¯ PV æ›¿æ¢é€»è¾‘æ˜¯å¦ç”Ÿæ•ˆ
        # =========================================================
        is_controlled = (data.bus_type != 2) # 0=Slack, 1=PV
        
        # æ›¿æ¢å‰è®°å½•ä¸€ä¸‹ï¼Œçœ‹çœ‹æœ‰å¤šå°‘è¯¯å·®
        diff_before = (pred_vm[is_controlled] - true_vm[is_controlled]).abs().mean().item()
        
        # --- æ‰§è¡Œæ›¿æ¢ ---
        pred_vm[is_controlled] = true_vm[is_controlled]
        
        # æ›¿æ¢åæ£€æŸ¥ï¼šè¯¯å·®å¿…é¡»æ˜¯ 0.0
        diff_after = (pred_vm[is_controlled] - true_vm[is_controlled]).abs().max().item()
        if diff_after > 1e-5:
            print(f"âš ï¸ è­¦å‘Šï¼Batch {batch_idx} æ›¿æ¢å¤±è´¥ï¼PVèŠ‚ç‚¹æœ€å¤§æ®‹å·®: {diff_after}")
        # =========================================================

        # 3. é€æ ·æœ¬æº¯æº
        # æ‰¾å‡ºæ¯ä¸ªå›¾çš„ Max ç”µå‹æ˜¯ç”±å“ªä¸ªèŠ‚ç‚¹è´¡çŒ®çš„
        num_graphs = data.num_graphs
        for i in range(num_graphs):
            # è·å–å±äºè¿™å¼ å›¾çš„æ‰€æœ‰èŠ‚ç‚¹ç´¢å¼•
            node_indices = torch.where(data.batch == i)[0]
            
            # å–å‡ºè¿™å¼ å›¾çš„ç”µå‹
            local_pred_vm = pred_vm[node_indices]
            local_true_vm = true_vm[node_indices]
            local_types   = data.bus_type[node_indices]
            
            # --- å…³é”®ï¼šå¯»æ‰¾ Max çš„â€œè‚‡äº‹è€…â€ ---
            # æ‰¾åˆ°é¢„æµ‹ç”µå‹æœ€é«˜çš„é‚£ä¸ªèŠ‚ç‚¹çš„å±€éƒ¨ç´¢å¼•
            max_val_pred, argmax_idx = torch.max(local_pred_vm, dim=0)

            max_val_true, true_argmax_idx = torch.max(local_true_vm, dim=0)
            max_node_type_true = local_types[true_argmax_idx].item()
            
            # æŸ¥æˆ·å£ï¼šè¿™ä¸ªèŠ‚ç‚¹æ˜¯ä»€ä¹ˆç±»å‹ï¼Ÿ
            max_node_type = local_types[argmax_idx].item() # 0, 1, or 2
            
            # å¯¹åº”çš„çœŸå®ç”µå‹æ˜¯å¤šå°‘ï¼Ÿ
            max_val_true_at_that_node = local_true_vm[argmax_idx].item()
            
            # è¿™å¼ å›¾çœŸå®çš„æœ€é«˜ç”µå‹æ˜¯å¤šå°‘ï¼ˆå¯èƒ½ä¸æ˜¯åŒä¸€ä¸ªç‚¹ï¼‰
            global_true_max = local_true_vm.max().item()
            
            records.append({
                'pred_max': max_val_pred.item(),
                'true_max': global_true_max,
                'source_type': max_node_type, # 0=Slack, 1=PV, 2=PQ
                'is_aligned': abs(max_val_pred.item() - global_true_max) < 1e-4,
                'true_max_source_type': max_node_type_true # è®°å½•çœŸå€¼çš„å† å†›ç±»å‹
            })
            
    print(f"âœ… è¯Šæ–­å®Œæˆï¼Œå…±åˆ†æ {len(records)} ä¸ªæ ·æœ¬ã€‚æ­£åœ¨ç»˜å›¾...")
    plot_diagnosis_results(records)

def plot_diagnosis_results(records):
    # è½¬æ¢ä¸º Numpy æ–¹ä¾¿åˆ‡ç‰‡
    pred_max = np.array([r['pred_max'] for r in records])
    true_max = np.array([r['true_max'] for r in records])
    source_type = np.array([r['source_type'] for r in records])
    
    # åˆ†ç±»
    mask_slack = (source_type == 0)
    mask_pv    = (source_type == 1)
    mask_pq    = (source_type == 2)
    
    plt.figure(figsize=(10, 10))
    
    # 1. ç”» Slack è´¡çŒ®çš„ Max (åº”è¯¥åœ¨çº¿ä¸Š)
    plt.scatter(true_max[mask_slack], pred_max[mask_slack], 
                c='green', s=20, alpha=0.6, label=f'Max from Slack (N={mask_slack.sum()})')
    
    # 2. ç”» PV è´¡çŒ®çš„ Max (åº”è¯¥åœ¨çº¿ä¸Š)
    plt.scatter(true_max[mask_pv], pred_max[mask_pv], 
                c='blue', s=20, alpha=0.6, label=f'Max from PV (N={mask_pv.sum()})')
    
    # 3. ç”» PQ è´¡çŒ®çš„ Max (å¯èƒ½æ˜¯ç½ªé­ç¥¸é¦–)
    plt.scatter(true_max[mask_pq], pred_max[mask_pq], 
                c='red', s=20, alpha=0.5, label=f'Max from PQ (N={mask_pq.sum()})')
    
    # ç”»å¯¹è§’çº¿å’Œè¾¹ç•Œ
    plt.plot([0.9, 1.2], [0.9, 1.2], 'k--', linewidth=1)
    plt.axhline(1.05, color='gray', linestyle='--')
    plt.axvline(1.05, color='gray', linestyle='--')
    
    plt.title("Diagnostics: Which Node Type Determines Pred Max Voltage?")
    plt.xlabel("True Max Voltage of Graph")
    plt.ylabel("Pred Max Voltage of Graph")
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    save_path = 'max_voltage_source_debug.png'
    plt.savefig(save_path, dpi=150)
    print(f"ğŸ–¼ï¸ è¯Šæ–­å›¾å·²ä¿å­˜è‡³: {save_path}")
    
    # --- æ–‡å­—ç»Ÿè®¡æŠ¥å‘Š ---
    print("\n" + "="*50)
    print("ğŸ“Š Max Voltage æ¥æºç»Ÿè®¡æŠ¥å‘Š")
    print("="*50)
    total = len(records)
    print(f"æ€»æ ·æœ¬æ•°: {total}")
    
    # ç»Ÿè®¡ PQ å¯¼è‡´çš„è¿‡å†²
    # è¿‡å†²å®šä¹‰ï¼šPred Max > True Max + 0.001 (å…è®¸ä¸€ç‚¹æµ®ç‚¹è¯¯å·®)
    overshoot = (pred_max > true_max + 0.001)
    
    n_pq = mask_pq.sum()
    n_pq_overshoot = (mask_pq & overshoot).sum()
    
    print(f"1. ç”± PQ èŠ‚ç‚¹å†³å®šæœ€é«˜ç”µå‹çš„æ ·æœ¬æ•°: {n_pq} ({n_pq/total:.2%})")
    print(f"   -> å…¶ä¸­å‘ç”Ÿè¿‡å†²(Pred > True)çš„æ•°é‡: {n_pq_overshoot}")
    if n_pq > 0:
        print(f"   -> PQè¿‡å†²ç‡: {n_pq_overshoot/n_pq:.2%}")
        
    n_pv_slack = mask_pv.sum() + mask_slack.sum()
    n_pv_error = ((mask_pv | mask_slack) & (np.abs(pred_max - true_max) > 1e-4)).sum()
    print(f"2. ç”± PV/Slack å†³å®šæœ€é«˜ç”µå‹çš„æ ·æœ¬æ•°: {n_pv_slack} ({n_pv_slack/total:.2%})")
    print(f"   -> å…¶ä¸­åç¦»å¯¹è§’çº¿çš„æ•°é‡: {n_pv_error} (åº”è¯¥æ¥è¿‘0)")
    
    if n_pv_error > 0:
        print("âš ï¸ è­¦å‘Šï¼šæ£€æµ‹åˆ° PV/Slack èŠ‚ç‚¹ä¸åœ¨å¯¹è§’çº¿ä¸Šï¼è¿™æ„å‘³ç€æ›¿æ¢é€»è¾‘ä»£ç æ²¡ç”Ÿæ•ˆï¼Œæˆ–è€…ç»Ÿè®¡é€»è¾‘æœ‰Bugã€‚")
    else:
        print("âœ… ç¡®è®¤ï¼šæ‰€æœ‰ç”± PV/Slack ä¸»å¯¼çš„æ ·æœ¬å‡å®Œç¾è½åœ¨å¯¹è§’çº¿ä¸Šã€‚")

    true_pq_wins = np.array([r['true_max_source_type'] == 2 for r in records]).sum()
    print(f"ğŸ” çœŸç›¸æ ¸æŸ¥ï¼šåœ¨çœŸå®æ ‡ç­¾(Ground Truth)ä¸­ï¼ŒPQèŠ‚ç‚¹æ˜¯æœ€é«˜ç”µå‹çš„æ¯”ä¾‹: {true_pq_wins/total:.2%}")

# ==========================================
# 2. æ ¸å¿ƒè¯„ä¼°å‡½æ•° (å·²ä¿®æ”¹ï¼šå¼•å…¥å·²çŸ¥èŠ‚ç‚¹ç”µå‹æ›¿æ¢)
# ==========================================

@torch.no_grad()
def evaluate_full_metrics(model, loader, device, xymean, xystd, edgemean, edgestd):
    model.eval()
    
    metrics = {
        'num_samples': 0,
        'mae_vm': 0., 
        'all_gt_labels': [],
        'all_pred_labels': [],
        'fp_details': [], 
        'fn_details': [],
        # è¿™é‡Œå­˜çš„å°†æ˜¯ (Global_Min, PQ_Max, True_Global_Min, True_PQ_Max)
        'all_samples_extremes': [] 
    }
    
    ef_mean = xymean[:, 2:4].to(device)
    ef_std  = xystd[:, 2:4].to(device)
    
    BIAS_CORRECTION = 0.00  
    
    LIMIT_LOW = 0.95
    LIMIT_PQ_HIGH = 1.05

    for data in loader:
        data = data.to(device)
        out = model(data)
        
        # 1. åå½’ä¸€åŒ–
        target_ef = data.y[:, 2:4]
        pred_real = out * (ef_std + 1e-7) + ef_mean
        target_real = target_ef * (ef_std + 1e-7) + ef_mean
        
        # 2. è®¡ç®—èŠ‚ç‚¹ Vm
        pred_vm, _ = rect_to_polar(pred_real[:, 0], pred_real[:, 1])
        true_vm, _ = rect_to_polar(target_real[:, 0], target_real[:, 1])
        
        pred_vm = pred_vm - BIAS_CORRECTION
        
        # ---------------------------------------------------------
        # ã€åˆ¤å®šé€»è¾‘é‡æ„ã€‘
        # ---------------------------------------------------------
        if hasattr(data, 'bus_type'):
            is_pq = (data.bus_type == 2)
        else:
            # é˜²å¾¡æ€§ä»£ç 
            is_pq = torch.ones_like(pred_vm, dtype=torch.bool)
        
        # A. ä½å‹åˆ¤å®šï¼šå…¨ç½‘ä»»ä½•èŠ‚ç‚¹ < 0.95
        # (pred_vm < 0.95)
        v_unsafe_low_node = (pred_vm < LIMIT_LOW)
        
        # B. é«˜å‹åˆ¤å®šï¼šä»… PQ èŠ‚ç‚¹ > 1.05
        # (pred_vm > 1.05) AND (is_pq)
        v_unsafe_high_node = (pred_vm > LIMIT_PQ_HIGH) & is_pq
        
        # C. ç»¼åˆèŠ‚ç‚¹è¶Šé™æƒ…å†µ
        v_unsafe_node = v_unsafe_low_node | v_unsafe_high_node
        
        # ---------------------------------------------------------
        
        # 4. é€æ ·æœ¬åˆ†æ
        for i in range(data.num_graphs):
            node_mask = (data.batch == i)
            
            # --- å…³é”®ï¼šæ„å»º PQ æ©ç  ---
            # å½“å‰å›¾ä¸­ï¼Œæ—¢å±äºè¯¥å›¾ï¼Œåˆæ˜¯ PQ ç±»å‹çš„èŠ‚ç‚¹
            pq_mask_in_graph = node_mask & is_pq
            
            # A. Ground Truth
            gt_status = 1 if data.label[i].item() > 0 else 0
            metrics['all_gt_labels'].append(gt_status)
            
            # B. AI Prediction
            ai_v_unsafe = v_unsafe_node[node_mask].any().item()
            pred_status = 1 if ai_v_unsafe else 0
            metrics['all_pred_labels'].append(pred_status)
            
            # C. æ”¶é›†æå€¼æ•°æ® (æ ¸å¿ƒä¿®æ”¹)
            # ---------------------------------------------
            # Min: å–å…¨ç½‘æœ€ä½ (åŒ…æ‹¬ PVï¼Œå› ä¸º PV è·Œè½ä¹Ÿç®—æ•…éšœ)
            p_curr_all = pred_vm[node_mask]
            t_curr_all = true_vm[node_mask]
            p_min = p_curr_all.min().item()
            t_min = t_curr_all.min().item()
            
            # Max: åªå– PQ èŠ‚ç‚¹çš„æœ€é«˜å€¼ï¼
            # åªæœ‰å½“è¯¥å›¾æœ‰ PQ èŠ‚ç‚¹æ—¶æ‰è®¡ç®— (æ­£å¸¸éƒ½æœ‰)
            if pq_mask_in_graph.any():
                p_curr_pq = pred_vm[pq_mask_in_graph]
                t_curr_pq = true_vm[pq_mask_in_graph]
                p_max = p_curr_pq.max().item()
                t_max = t_curr_pq.max().item()
            else:
                # æç«¯æƒ…å†µ fallback
                p_max = -1.0 
                t_max = -1.0
            # ---------------------------------------------
            
            metrics['all_samples_extremes'].append((p_min, p_max, t_min, t_max))

            # D. è¯Šæ–­è¯¯æŠ¥ (FP)
            if gt_status == 0 and pred_status == 1:
                # è®°å½•è¯¯å·®æ—¶ï¼Œç®€å•è®°å½•æœ€å¤§ç»å¯¹è¯¯å·®å³å¯
                max_err = (t_curr_all - p_curr_all).abs().max().item()
                metrics['fp_details'].append((t_min, t_max, p_min, p_max, max_err))
                
            # E. è¯Šæ–­æ¼æŠ¥ (FN)
            if gt_status == 1 and pred_status == 0:
                max_err = (t_curr_all - p_curr_all).abs().max().item()
                metrics['fn_details'].append((t_min, t_max, p_min, p_max, max_err))

        # 5. MAE ç»Ÿè®¡ (åªç»Ÿè®¡ PQï¼Œåæ˜ è´Ÿè½½ä¾§ç²¾åº¦)
        m_sum = is_pq.sum().item() + 1e-6
        metrics['mae_vm'] += ((pred_vm - true_vm).abs() * is_pq).sum().item() / m_sum * data.num_graphs
        metrics['num_samples'] += data.num_graphs

    metrics['mae_vm'] /= metrics['num_samples']
    return metrics

# ==========================================
# 3. æ··åˆç­–ç•¥æ¨¡æ‹Ÿå‡½æ•° (ä¿æŒä¸å˜)
# ==========================================
def simulate_hybrid_strategy(metrics, limit_low=0.95, limit_high=1.05):
    data = np.array(metrics['all_samples_extremes']) 
    p_min, p_max = data[:, 0], data[:, 1]
    t_min, t_max = data[:, 2], data[:, 3]
    
    actual_unsafe = (t_min < limit_low) | (t_max > limit_high)
    actual_safe = ~actual_unsafe
    total_samples = len(p_min)

    print("\n" + "="*80)
    print("ğŸ¤– AI + ç‰©ç†æ··åˆç­–ç•¥æ¨¡æ‹Ÿ (åŒå‘æ£€æµ‹)")
    print("="*80)
    
    margins = [0.005, 0.010, 0.015, 0.020, 0.025]
    print(f"{'Margin':<8} | {'éœ€é‡ç®—(æˆæœ¬)':<15} | {'å‰©ä½™æ¼æŠ¥(FN)':<12} | {'çº¢åŒºè¯¯æŠ¥(FP)':<12}")
    print("-" * 80)
    
    for margin in margins:
        l_gray_start, l_gray_end = limit_low - margin, limit_low + margin
        h_gray_start, h_gray_end = limit_high - margin, limit_high + margin
        
        # 1. ç°åŒº (é‡ç®—)
        mask_recalc = ((p_min >= l_gray_start) & (p_min <= l_gray_end)) | \
                      ((p_max >= h_gray_start) & (p_max <= h_gray_end))
        n_recalc = np.sum(mask_recalc)
        
        # 2. ç»¿åŒº (AI æ”¾è¡Œ)
        mask_green = (p_min > l_gray_end) & (p_max < h_gray_start)
        
        # 3. çº¢åŒº (AI æŠ¥è­¦)
        mask_red = (p_min < l_gray_start) | (p_max > h_gray_end)
        
        n_crit_fn = np.sum(mask_green & actual_unsafe)
        n_red_fp = np.sum(mask_red & actual_safe)
        
        print(f"+/-{margin:<.3f} | {n_recalc/total_samples:<6.2%} ({n_recalc})   | {n_crit_fn:<12d} | {n_red_fp:<12d}")
    print("-" * 80)

# ==========================================
# 4. åˆ†å¸ƒè¯Šæ–­å‡½æ•° (ä¿æŒä¸å˜)
# ==========================================
def plot_distribution_debug(metrics, save_path='dist_debug.png'):
    # ... (ä»£ç ä¿æŒä¸å˜ï¼Œç›´æ¥å¤åˆ¶å³å¯)
    # ç•¥å»ä»¥èŠ‚çœç¯‡å¹…ï¼Œè¯·ä¿ç•™ä½ åŸæœ‰çš„ç»˜å›¾ä»£ç 
    print("\n" + "="*50)
    print("ğŸ”¬ æ­£åœ¨è¿›è¡Œåˆ†å¸ƒâ€œå°¸æ£€â€è¯Šæ–­...")
    if len(metrics['all_samples_extremes']) == 0: return

    data = np.array(metrics['all_samples_extremes'])
    p_min, p_max = data[:, 0], data[:, 1]
    t_min, t_max = data[:, 2], data[:, 3]

    sns.set_theme(style="whitegrid")
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))

    # 1. Min Voltage åˆ†å¸ƒ (Sag)
    sns.kdeplot(t_min, ax=axes[0,0], color='blue', fill=True, label='True Min', clip=(0.8, 1.05))
    sns.kdeplot(p_min, ax=axes[0,0], color='red', fill=True, label='Pred Min', clip=(0.8, 1.05))
    axes[0,0].axvline(0.95, color='k', linestyle='--', label='Limit 0.95')
    axes[0,0].set_title('Distribution of MIN Voltage')
    axes[0,0].legend()

    # 2. Max Voltage åˆ†å¸ƒ (Swell)
    sns.kdeplot(t_max, ax=axes[0,1], color='blue', fill=True, label='True Max', clip=(0.95, 1.2))
    sns.kdeplot(p_max, ax=axes[0,1], color='red', fill=True, label='Pred Max', clip=(0.95, 1.2))
    axes[0,1].axvline(1.05, color='k', linestyle='--', label='Limit 1.05')
    axes[0,1].set_title('Distribution of MAX Voltage')
    axes[0,1].legend()

    # 3. æ•£ç‚¹å›¾ Min
    axes[1,0].scatter(t_min, p_min, alpha=0.3, s=5, color='purple')
    axes[1,0].plot([0.8, 1.1], [0.8, 1.1], 'k--')
    axes[1,0].axhline(0.95, color='r', linestyle='--')
    axes[1,0].axvline(0.95, color='b', linestyle='--')
    axes[1,0].set_title('Scatter: True vs Pred Min')
    axes[1,0].set_xlabel('True Min')
    axes[1,0].set_ylabel('Pred Min')

    # 4. æ•£ç‚¹å›¾ Max
    axes[1,1].scatter(t_max, p_max, alpha=0.3, s=5, color='green')
    axes[1,1].plot([0.9, 1.2], [0.9, 1.2], 'k--')
    axes[1,1].axhline(1.05, color='r', linestyle='--')
    axes[1,1].axvline(1.05, color='b', linestyle='--')
    axes[1,1].set_title('Scatter: True vs Pred Max')
    axes[1,1].set_xlabel('True Max')
    axes[1,1].set_ylabel('Pred Max')

    plt.tight_layout()
    plt.savefig(save_path)
    print(f"[Plot] åˆ†å¸ƒè¯Šæ–­å›¾å·²ä¿å­˜è‡³: {save_path}")

    # --- å…³é”®ç»Ÿè®¡ ---
    total = len(p_min)
    # ç»Ÿè®¡è½åœ¨ç°åŒº (0.945~0.955) å’Œ (1.045~1.055) çš„æ¯”ä¾‹
    in_low_gray = np.sum((p_min >= 0.945) & (p_min <= 0.955))
    in_high_gray = np.sum((p_max >= 1.045) & (p_max <= 1.055))

    print("\n[ğŸ“Š ç°åŒºå †ç§¯åˆ†æ]")
    print(f"Pred Min è½åœ¨ [0.945, 0.955] (ä½å‹è¾¹ç•Œ) çš„æ¯”ä¾‹: {in_low_gray/total:.2%} ({in_low_gray})")
    print(f"Pred Max è½åœ¨ [1.045, 1.055] (é«˜å‹è¾¹ç•Œ) çš„æ¯”ä¾‹: {in_high_gray/total:.2%} ({in_high_gray})")
    print("--> å¦‚æœè¿™ä¸¤ä¸ªæ¯”ä¾‹å¾ˆé«˜ï¼Œè¯´æ˜å¤§é‡æ ·æœ¬å¡åœ¨è¾¹ç•Œä¸Šï¼Œå¯¼è‡´æ··åˆç­–ç•¥å¤±æ•ˆ(å¿…é¡»é‡ç®—)ã€‚")

# ==========================================
# 5. ä¸»ç¨‹åº
# ==========================================
def main():
    run_id = '20260108-2395'  # è®°å¾—æ”¹æˆä½ æœ€æ–°è®­ç»ƒçš„ ID (é—¨æ§+NodeIDç‰ˆ)
    case_name = '118v2_30w_n1' 
    
    args = argument_parser()
    args.case = case_name 
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # 1. åŠ è½½å‚æ•°
    data_param_path = os.path.join(args.data_dir, 'params', f'data_params_{run_id}.pt')
    if not os.path.exists(data_param_path):
        print(f"Error: æ‰¾ä¸åˆ°å‚æ•°æ–‡ä»¶ {data_param_path}")
        return
    data_param = torch.load(data_param_path, map_location='cpu')
    
    # 2. åŠ è½½æ•°æ®
    print("Loading Test Data...")
    testset = PowerFlowData(root=args.data_dir, case=args.case,
                            split=[.5, .2, .3], task='test',
                            xymean=data_param['xymean'], xystd=data_param['xystd'],
                            edgemean=data_param['edgemean'], edgestd=data_param['edgestd'])
    loader = DataLoader(testset, batch_size=args.batch_size, shuffle=False)
    
    # 3. åŠ è½½æ¨¡å‹
    # æ³¨æ„ï¼šç¡®ä¿è¿™é‡Œå®ä¾‹åŒ–æ¨¡å‹æ—¶å‚æ•°ä¸ä½ è®­ç»ƒæ—¶ä¸€è‡´ (ä¾‹å¦‚ num_nodes)
    node_in, _, edge_dim = testset.get_data_dimensions()
    model = MaskEmbdMultiMPN_GPS(nfeature_dim=node_in, efeature_dim=edge_dim, output_dim=2, 
                                 hidden_dim=args.hidden_dim, n_gnn_layers=args.n_gnn_layers,
                                 num_nodes=118).to(device) # åŠ ä¸Š num_nodes=118
    model, _ = load_model(model, run_id, device)
    
    # 4. è¿è¡Œè¯„ä¼°
    print("Running Full Evaluation...")
    res = evaluate_full_metrics(model, loader, device, 
                                data_param['xymean'], data_param['xystd'],
                                data_param['edgemean'], data_param['edgestd'])
    
    # debug_max_voltage_source(model, loader, device, data_param['xymean'], data_param['xystd'])
    
    # 5. è¾“å‡ºæŠ¥å‘Š
    gt = np.array(res['all_gt_labels'])
    pred = np.array(res['all_pred_labels'])
    cm = confusion_matrix(gt, pred)
    # å¤„ç†å¯èƒ½çš„ shape ä¸åŒ¹é… (ä¾‹å¦‚æµ‹è¯•é›†å…¨æ˜¯å®‰å…¨æ ·æœ¬)
    if cm.size == 1:
        tn = cm[0,0]
        fp, fn, tp = 0, 0, 0
    else:
        tn, fp, fn, tp = cm.ravel()

    print("\n" + "="*50)
    print(f"ğŸ›¡ï¸  N-1 å®‰å…¨åˆ¤å®šè¯„ä¼° (PVä¿®æ­£å)")
    print(f"  å‡†ç¡®è¯†åˆ«å®‰å…¨ (TN): {tn}")
    print(f"  è¯¯æŠ¥ (FP): {fp}")
    print(f"  æ¼æŠ¥ (FN): {fn}")
    print(f"  æ­£ç¡®è¯†åˆ«æ•…éšœ (TP): {tp}")
    print(f"  Recall: {tp/(tp+fn+1e-9):.2%}")
    print(f"  FPR   : {fp/(fp+tn+1e-9):.2%}")
    print("="*50)
    
    simulate_hybrid_strategy(res)
    plot_distribution_debug(res) # éœ€è¦è¡¥å…¨ä¸Šé¢çš„å‡½æ•°æ‰èƒ½è¿è¡Œç»˜å›¾

if __name__ == "__main__":
    main()