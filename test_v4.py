import os
import logging
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from torch_geometric.loader import DataLoader
from sklearn.metrics import confusion_matrix
from collections import Counter

# å¯¼å…¥ä½ çš„è‡ªå®šä¹‰æ¨¡å—
from datasets.PowerFlowData import PowerFlowData 
from networks.MPN import MaskEmbdMultiMPN_GPS
from utils.evaluation import load_model
from utils.argument_parser import argument_parser

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ==========================================
# 1. ç‰©ç†è¾…åŠ©å‡½æ•°
# ==========================================

def rect_to_polar(e, f):
    """å°†ç›´è§’åæ ‡é¢„æµ‹å€¼è½¬æ¢ä¸ºæåæ ‡ (Vm, Va_degree)"""
    vm = torch.sqrt(e**2 + f**2 + 1e-12)
    va_rad = torch.atan2(f, e)
    va_deg = va_rad * (180.0 / torch.pi)
    return vm, va_deg

# ==========================================
# 2. æ ¸å¿ƒè¯„ä¼°å‡½æ•° (é›†æˆ Bias Correction + æ··åˆç­–ç•¥æ•°æ®æ”¶é›†)
# ==========================================

@torch.no_grad()
def evaluate_full_metrics(model, loader, device, xymean, xystd, edgemean, edgestd):
    model.eval()
    
    metrics = {
        'num_samples': 0,
        'mae_vm': 0., 
        'all_gt_labels': [],
        'all_pred_labels': [],
        'fp_details': [], # è¯¯æŠ¥æ ·æœ¬è¯¦æƒ…
        'fn_details': [], # æ¼æŠ¥æ ·æœ¬è¯¦æƒ…
        'all_samples_min_vm': [] # å­˜å‚¨æ‰€æœ‰æ ·æœ¬çš„ (Pred_Min, True_Min)ï¼Œç”¨äºæ··åˆç­–ç•¥åˆ†æ
    }
    
    ef_mean = xymean[:, 2:4].to(device)
    ef_std  = xystd[:, 2:4].to(device)
    
    # è®¾å®šæ ¡å‡†å‚æ•°
    BIAS_CORRECTION = 0.002  # å¼ºåˆ¶å‹ä½é¢„æµ‹ç”µå‹
    SAFETY_LIMIT = 0.95      # å®‰å…¨é˜ˆå€¼

    for data in loader:
        data = data.to(device)
        out = model(data) # é¢„æµ‹ e, f
        
        # 1. åå½’ä¸€åŒ–
        target_ef = data.y[:, 2:4]
        pred_real = out * (ef_std + 1e-7) + ef_mean
        target_real = target_ef * (ef_std + 1e-7) + ef_mean
        
        # 2. è®¡ç®—èŠ‚ç‚¹ Vm
        pred_vm, _ = rect_to_polar(pred_real[:, 0], pred_real[:, 1])
        true_vm, _ = rect_to_polar(target_real[:, 0], target_real[:, 1])
        
        # ==========================================
        # ğŸ’‰ ã€æ ¸å¿ƒä¿®æ”¹ã€‘ åå·®æ ¡å‡† (Bias Correction)
        # ==========================================
        pred_vm = pred_vm - BIAS_CORRECTION
        # ==========================================
        
        # 3. å®‰å…¨åˆ¤å®šé€»è¾‘ (åªçœ‹ä½å‹ < 0.95)
        # æ³¨æ„ï¼šè¿™é‡Œçš„é«˜å‹åˆ¤å®šè¢«ç§»é™¤äº†ï¼Œä¸ºäº†å‡å°‘ FP
        v_unsafe_node = (pred_vm < SAFETY_LIMIT)
        
        # 4. é€æ ·æœ¬åˆ†æ
        for i in range(data.num_graphs):
            node_mask = (data.batch == i)
            
            # A. Ground Truth (åªè¦ Dataset label > 0 å°±ç®—ä¸å®‰å…¨)
            gt_status = 1 if data.label[i].item() > 0 else 0
            metrics['all_gt_labels'].append(gt_status)
            
            # B. AI Prediction
            ai_v_unsafe = v_unsafe_node[node_mask].any().item()
            pred_status = 1 if ai_v_unsafe else 0
            metrics['all_pred_labels'].append(pred_status)
            
            # C. æ”¶é›†æ··åˆç­–ç•¥æ‰€éœ€æ•°æ® (Min Vm)
            p_min = pred_vm[node_mask].min().item()
            t_min = true_vm[node_mask].min().item()
            metrics['all_samples_min_vm'].append((p_min, t_min))

            # D. è¯Šæ–­è¯¯æŠ¥ (FP)
            if gt_status == 0 and pred_status == 1:
                max_err = (true_vm[node_mask] - pred_vm[node_mask]).abs().max().item()
                metrics['fp_details'].append((t_min, p_min, max_err))
                
            # E. è¯Šæ–­æ¼æŠ¥ (FN)
            if gt_status == 1 and pred_status == 0:
                max_err = (true_vm[node_mask] - pred_vm[node_mask]).abs().max().item()
                metrics['fn_details'].append((t_min, p_min, max_err))

        # 5. åŸºç¡€è¯¯å·®ç»Ÿè®¡
        node_mask_all = data.pred_mask[:, 2] 
        m_sum = node_mask_all.sum().item() + 1e-6
        metrics['mae_vm'] += ((pred_vm - true_vm).abs() * node_mask_all).sum().item() / m_sum * data.num_graphs
        metrics['num_samples'] += data.num_graphs

    metrics['mae_vm'] /= metrics['num_samples']
    return metrics

# ==========================================
# 3. æ··åˆç­–ç•¥æ¨¡æ‹Ÿå‡½æ•° (Hybrid Simulation)
# ==========================================
def simulate_hybrid_strategy(metrics, base_threshold=0.95):
    all_vm = np.array(metrics['all_samples_min_vm']) # Shape: [N, 2]
    pred_min = all_vm[:, 0]
    true_min = all_vm[:, 1]
    
    # çœŸå®çš„â€œä¸å®‰å…¨â€æ ·æœ¬ (True Min < 0.95)
    actual_unsafe = (true_min < base_threshold)
    total_samples = len(pred_min)

    print("\n" + "="*60)
    print("ğŸ¤– AI + ç‰©ç†æ··åˆç­–ç•¥æ¨¡æ‹Ÿ (AI-Physics Hybrid Solver)")
    print("   ç­–ç•¥: é¢„æµ‹å€¼åœ¨ [0.95-Margin, 0.95+Margin] ä¹‹é—´çš„æ ·æœ¬ï¼Œ")
    print("         äº¤ç»™ç‰©ç†æ±‚è§£å™¨é‡ç®—(è€—æ—¶)ï¼Œå…¶ä½™ç›´æ¥ä¿¡AI(æé€Ÿ)ã€‚")
    print("="*60)
    
    # æµ‹è¯•ä¸åŒçš„è£•åº¦ (Margin)
    margins = [0.005, 0.010, 0.015, 0.020, 0.025]
    
    print(f"{'Margin':<10} | {'åŒºé—´ (é‡ç®—åŒº)':<20} | {'éœ€é‡ç®—æ¯”ä¾‹(æˆæœ¬)':<15} | {'å‰©ä½™æ¼æŠ¥(é£é™©)':<15} | {'è¯„ä»·'}")
    print("-" * 90)
    
    for margin in margins:
        lower = base_threshold - margin
        upper = base_threshold + margin
        
        # 1. ç°åŒº (é‡ç®—)
        mask_gray = (pred_min >= lower) & (pred_min <= upper)
        n_recalc = np.sum(mask_gray)
        ratio_recalc = n_recalc / total_samples
        
        # 2. ç»¿åŒº (AI æ”¾è¡Œ)
        # AI è®¤ä¸º > upper (éå¸¸å®‰å…¨)ï¼Œç›´æ¥æ”¾è¡Œ
        mask_green = (pred_min > upper)
        
        # 3. æ¶æ€§æ¼æŠ¥ (Critical FN)
        # AI æ”¾è¡Œäº†ï¼Œä½†å…¶å®æ˜¯å±é™©çš„
        n_critical_fn = np.sum(mask_green & actual_unsafe)
        
        status = "âœ… å®Œç¾" if n_critical_fn == 0 else "âš ï¸ æœ‰é£é™©"
        
        print(f"+/- {margin:<5.3f} | [{lower:.3f}, {upper:.3f}]      | {ratio_recalc:<6.2%} ({n_recalc})   | {n_critical_fn:<13d}   | {status}")

    print("-" * 90)

# ==========================================
# 4. ç»˜å›¾å‡½æ•°
# ==========================================
def plot_fn_analysis(fn_details, save_path='fn_analysis.png'):
    if len(fn_details) == 0: return
    fn_array = np.array(fn_details)
    true_min = fn_array[:, 0]
    pred_min = fn_array[:, 1]

    sns.set_theme(style="whitegrid")
    plt.figure(figsize=(8, 8))
    
    plt.scatter(true_min, pred_min, alpha=0.6, color='orange', label='FN Samples')
    
    # ç”»çº¿
    plt.plot([0.85, 1.0], [0.85, 1.0], 'k--', label='Ideal x=y')
    plt.axhline(0.95, color='r', linestyle='--', label='AI Threshold (0.95)')
    plt.axvline(0.95, color='b', linestyle='--', label='True Threshold (0.95)')
    
    # å¡«å……æ¶æ€§æ¼æŠ¥åŒºåŸŸ
    plt.fill_between([0.85, 0.95], 0.95, 1.0, color='red', alpha=0.1, label='Critical FN Zone')

    plt.title('False Negative Analysis\n(Why AI missed them?)')
    plt.xlabel('True Min Voltage (Unsafe)')
    plt.ylabel('Pred Min Voltage (Safe)')
    plt.legend()
    plt.savefig(save_path)
    print(f"[Plot] æ¼æŠ¥åˆ†å¸ƒå›¾å·²ä¿å­˜è‡³: {save_path}")

# ==========================================
# 5. ä¸»ç¨‹åº
# ==========================================
def main():
    # ---------------------------------------------
    # è¯·ç¡®ä¿è¿™é‡Œçš„ ID å’Œ case åå­—æ˜¯å¯¹çš„
    run_id = '20251223-6480' 
    case_name = '118v_n1_train' 
    # ---------------------------------------------
    
    args = argument_parser()
    # å¼ºåˆ¶è¦†ç›– args ä»¥ä¾¿ç›´æ¥è¿è¡Œ
    args.case = case_name 
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # 1. åŠ è½½å‚æ•°
    data_param_path = os.path.join(args.data_dir, 'params', f'data_params_{run_id}.pt')
    if not os.path.exists(data_param_path):
        print(f"Error: æ‰¾ä¸åˆ°å‚æ•°æ–‡ä»¶ {data_param_path}")
        return
    data_param = torch.load(data_param_path, map_location='cpu')
    
    # 2. åŠ è½½æ•°æ®
    print("Loading Test Data...")
    testset = PowerFlowData(root=args.data_dir, case=args.case,
                            split=[.5, .2, .3], task='test',
                            xymean=data_param['xymean'], xystd=data_param['xystd'],
                            edgemean=data_param['edgemean'], edgestd=data_param['edgestd'])
    loader = DataLoader(testset, batch_size=args.batch_size, shuffle=False)
    
    # 3. åŠ è½½æ¨¡å‹
    node_in, _, edge_dim = testset.get_data_dimensions()
    model = MaskEmbdMultiMPN_GPS(nfeature_dim=node_in, efeature_dim=edge_dim, output_dim=2, 
                                 hidden_dim=args.hidden_dim, n_gnn_layers=args.n_gnn_layers).to(device)
    model, _ = load_model(model, run_id, device)
    
    # 4. è¿è¡Œè¯„ä¼°
    print("Running Evaluation (Bias Correction = -0.002)...")
    res = evaluate_full_metrics(model, loader, device, 
                                data_param['xymean'], data_param['xystd'],
                                data_param['edgemean'], data_param['edgestd'])
    
    # 5. è¾“å‡ºå¸¸è§„æŠ¥å‘Š
    gt = np.array(res['all_gt_labels'])
    pred = np.array(res['all_pred_labels'])
    cm = confusion_matrix(gt, pred)
    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (cm[0,0], 0, 0, 0)

    print("\n" + "="*50)
    print(f"ğŸ›¡ï¸  N-1 å®‰å…¨åˆ¤å®šè¯„ä¼° (æ ¡å‡†å)")
    print(f"  å‡†ç¡®è¯†åˆ«å®‰å…¨ (TN): {tn}")
    print(f"  è¯¯æŠ¥ (FP): {fp}  (Bias Correction helps here)")
    print(f"  æ¼æŠ¥ (FN): {fn}")
    print(f"  æ­£ç¡®è¯†åˆ«æ•…éšœ (TP): {tp}")
    print("-" * 30)
    print(f"  Recall (æ•æ‰ç‡): {tp/(tp+fn+1e-9):.2%} (Goal: >90%)")
    print(f"  FPR (è¯¯æŠ¥ç‡)   : {fp/(fp+tn+1e-9):.2%}")
    print("="*50)
    
    # 6. è¿è¡Œæ··åˆç­–ç•¥æ¨¡æ‹Ÿ
    simulate_hybrid_strategy(res)

    # 7. ç”»æ¼æŠ¥åˆ†æå›¾
    plot_fn_analysis(res['fn_details'])

if __name__ == "__main__":
    main()