import os
import torch
import numpy as np
import wandb
import argparse
import shutil
from datetime import datetime
from torch_geometric.loader import DataLoader

# --- é¡¹ç›®æ¨¡å—å¼•å…¥ ---
from datasets.PowerFlowData import PowerFlowData
# å¼•å…¥æ‰€æœ‰å¯èƒ½ç”¨åˆ°çš„æ¨¡å‹ï¼Œé˜²æ­¢æŠ¥é”™
from networks.MPN import (
    MaskEmbdMultiMPN_GPS, 
)
from utils.training import train_epoch
from utils.evaluation import evaluate_epoch
from utils.custom_loss_functions import RectangularMixedLoss

# ==============================================================================
# é»˜è®¤é…ç½®
# ==============================================================================
DEFAULT_PRETRAINED_ID = "20251220-8946"
DEFAULT_NEW_CASE      = "118v_n1_train" # ç¡®ä¿è¿™é‡Œæ˜¯ä½ ç”Ÿæˆçš„ N-1 æ•°æ®é›†åå­—
DEFAULT_GPU           = "cuda:1"
# ==============================================================================

def parse_args():
    parser = argparse.ArgumentParser(description="Fine-tune PowerFlowNet on N-1 Data")
    
    parser.add_argument('--pretrained-id', type=str, default=DEFAULT_PRETRAINED_ID)
    parser.add_argument('--data-dir', type=str, default='data')
    parser.add_argument('--models-dir', type=str, default='models')
    parser.add_argument('--params-dir', type=str, default='data/params')
    
    parser.add_argument('--case', type=str, default=DEFAULT_NEW_CASE)
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--batch-size', type=int, default=32)
    
    # [å…³é”®ä¿®æ”¹] é»˜è®¤ LR é™ä¸º 1e-5ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
    parser.add_argument('--lr', type=float, default=1e-5)
    
    parser.add_argument('--alpha', type=float, default=1.0)
    parser.add_argument('--beta', type=float, default=3e-4)
    parser.add_argument('--gamma', type=float, default=5e-3)
    parser.add_argument('--anchor', type=float, default=0.1)
    
    parser.add_argument('--wandb', action='store_true')
    # å¡«å…¥ä½ è‡ªå·±çš„ç”¨æˆ·åï¼Œæˆ–è€…è®¾ä¸º None
    parser.add_argument('--wandb-entity', type=str, default=None) 
    
    return parser.parse_args()

def load_normalization_stats(params_path):
    if not os.path.exists(params_path):
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°å½’ä¸€åŒ–å‚æ•°: {params_path}")
    print(f">>> â™»ï¸ ç»§æ‰¿å½’ä¸€åŒ–å‚æ•°: {params_path}")
    return torch.load(params_path, map_location='cpu')

def check_data_health(loader, device):
    """æ£€æŸ¥æ•°æ®é›†ä¸­æ˜¯å¦æœ‰ NaN"""
    print(">>> æ­£åœ¨æ£€æŸ¥æ•°æ®å¥åº·çŠ¶å†µ (NaN Check)...")
    for batch_idx, data in enumerate(loader):
        if torch.isnan(data.x).any() or torch.isnan(data.y).any():
            raise ValueError(f"âŒ æ•°æ®ä¸­å‘ç° NaNï¼Batch Index: {batch_idx}")
        if torch.isinf(data.x).any() or torch.isinf(data.y).any():
            raise ValueError(f"âŒ æ•°æ®ä¸­å‘ç° Infï¼Batch Index: {batch_idx}")
    print("âœ… æ•°æ®å¥åº·æ£€æŸ¥é€šè¿‡ï¼Œæ—  NaN/Infã€‚")

def main():
    args = parse_args()
    device = torch.device(DEFAULT_GPU if torch.cuda.is_available() else 'cpu')
    
    # 1. è·¯å¾„ä¸ID
    pretrained_model_path = os.path.join(args.models_dir, f'model_{args.pretrained_id}.pt')
    pretrained_params_path = os.path.join(args.params_dir, f'data_params_{args.pretrained_id}.pt')
    
    current_time = datetime.now().strftime("%m%d-%H%M")
    new_run_id = f"ft-{current_time}-from-{args.pretrained_id.split('-')[-1]}"
    save_path = os.path.join(args.models_dir, f'model_{new_run_id}.pt')
    
    print(f"{'='*60}")
    print(f"ğŸš€ å¯åŠ¨å¾®è°ƒ | åŸºç¡€æ¨¡å‹: {args.pretrained_id} | æ•°æ®: {args.case}")
    print(f"   æ–° Run ID: {new_run_id}")
    print(f"{'='*60}\n")

    # 2. åŠ è½½é¢„è®­ç»ƒ Checkpoint
    if not os.path.exists(pretrained_model_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨¡å‹: {pretrained_model_path}")
    
    checkpoint = torch.load(pretrained_model_path, map_location=device, weights_only=False)
    train_args = checkpoint['args']
    
    # 3. æ•°æ®é›†å‡†å¤‡ (å¿…é¡»ç»§æ‰¿å‚æ•°!)
    stats = load_normalization_stats(pretrained_params_path)
    
    print(f">>> åŠ è½½æ•°æ®é›†...")
    # æ³¨æ„ï¼šä¸€å®šè¦ç¡®ä¿ data/processed ä¸‹çš„æ—§ç¼“å­˜å·²è¢«åˆ é™¤ï¼Œå¦åˆ™ inherit stats æ— æ•ˆ
    trainset = PowerFlowData(
        root=args.data_dir, case=args.case, split=[.9, .05, .05], task='train',
        xymean=stats['xymean'], xystd=stats['xystd'], 
        edgemean=stats['edgemean'], edgestd=stats['edgestd']
    )
    valset = PowerFlowData(
        root=args.data_dir, case=args.case, split=[.9, .05, .05], task='val',
        xymean=stats['xymean'], xystd=stats['xystd'], 
        edgemean=stats['edgemean'], edgestd=stats['edgestd']
    )
    
    train_loader = DataLoader(trainset, batch_size=args.batch_size, shuffle=True)
    val_loader = DataLoader(valset, batch_size=args.batch_size, shuffle=False)

    # [æ–°å¢] è®­ç»ƒå‰è‡ªæ£€
    check_data_health(train_loader, device)

    # 4. æ¨¡å‹é‡å»º
    node_in_dim, _, edge_dim = trainset.get_data_dimensions()
    
    # åŠ¨æ€åŒ¹é…æ¨¡å‹ç±»
    model_name = getattr(train_args, 'model', 'MaskEmbdMultiMPN_GPS')
    print(f">>> é‡å»ºæ¨¡å‹æ¶æ„: {model_name}")
    
    # ç®€å•çš„å·¥å‚æ¨¡å¼
    if model_name == 'MaskEmbdMultiMPN_GPS': ModelClass = MaskEmbdMultiMPN_GPS
    else: raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹æ¶æ„: {model_name}")

    model = ModelClass(
        nfeature_dim=node_in_dim,
        efeature_dim=edge_dim,
        output_dim=2, 
        hidden_dim=train_args.hidden_dim,
        n_gnn_layers=train_args.n_gnn_layers,
        # å…¼å®¹æ—§å‚æ•°å¯èƒ½æ²¡æœ‰ nhead çš„æƒ…å†µ
        nhead=getattr(train_args, 'nhead', 4), 
        K=getattr(train_args, 'K', 3),
        dropout_rate=train_args.dropout_rate
    ).to(device)

    # åŠ è½½æƒé‡
    model.load_state_dict(checkpoint['model_state_dict'])
    print(">>> âœ… æƒé‡åŠ è½½å®Œæ¯•")

    # 5. Loss & Optimizer
    # æ³¨æ„ï¼šè¿™é‡Œç›´æ¥ä½¿ç”¨ç‰©ç† Loss
    loss_fn = RectangularMixedLoss(
        stats['xymean'], stats['xystd'], stats['edgemean'], stats['edgestd'],
        alpha=args.alpha, beta=args.beta, gamma=args.gamma, lambda_anchor=args.anchor
    ).to(device)
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)
    
    # ä½¿ç”¨ Cosine é€€ç«å¯èƒ½æ¯” ReduceLROnPlateau æ›´ç¨³
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=args.epochs, eta_min=1e-7
    )

    # 6. WandB
    if args.wandb:
        try:
            wandb.init(project="PowerFlowNet", entity=args.wandb_entity, name=new_run_id, config=train_args)
        except Exception as e:
            print(f"âš ï¸ WandB åˆå§‹åŒ–å¤±è´¥ (å¯èƒ½æ˜¯ç½‘ç»œæˆ–æƒé™é—®é¢˜)ï¼Œç»§ç»­è®­ç»ƒ... \n{e}")

    # 7. è®­ç»ƒå¾ªç¯
    best_val_loss = 1e9 
    
    for epoch in range(args.epochs):
        train_metrics = train_epoch(model, train_loader, loss_fn, optimizer, device)
        val_metrics = evaluate_epoch(model, val_loader, loss_fn, device)
        
        val_loss = val_metrics['total']
        scheduler.step() # Cosine ä¸éœ€è¦ä¼  loss
        lr_now = optimizer.param_groups[0]['lr']

        # æ—¥å¿—
        if args.wandb and wandb.run is not None:
            wandb.log({
                'epoch': epoch, 'lr': lr_now,
                'train_loss': train_metrics['total'], 'train_phys': train_metrics.get('phys', 0),
                'val_loss': val_loss, 'val_phys': val_metrics.get('phys', 0),
            })

        print(f"Ep {epoch+1}/{args.epochs} | LR={lr_now:.2e} | "
              f"Tr_Loss={train_metrics['total']:.5f} (Phys={train_metrics.get('phys',0):.3f}) | "
              f"Val={val_loss:.5f}")

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save({
                'epoch': epoch,
                'args': train_args, 
                'model_state_dict': model.state_dict(),
                'val_loss': best_val_loss
            }, save_path)

    print(f"\n>>> âœ… å¾®è°ƒç»“æŸï¼æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}")

if __name__ == '__main__':
    main()